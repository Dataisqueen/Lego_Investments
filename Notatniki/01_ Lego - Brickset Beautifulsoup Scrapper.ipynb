{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbab59f6",
   "metadata": {},
   "source": [
    "### SCRAPER DO BRICKSET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18bff4",
   "metadata": {},
   "source": [
    "Tutaj kod niestety nie odpali się elegancko - żeby wszystko się ładnie zebrało, musiałam pobrać dane indexów setów z innej strony,\n",
    "w międzyczasie pobrałam trochę setów innymi metodami. Ostatecznie musiałam zrobić listę zawierającą różnicę setów które już posiadam, \n",
    "a które jeszcze muszę mieć. Te dwie listy miałam w plikach csv na dysku.\n",
    "\n",
    "Kiedy miałam już wszystkie pliki CSV załadowałam je z poziomu Excela przez Power Query w jeden plik - dlatego nie ma później formuły w Pythonie, która je łączy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2455c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ścieżki do plików CSV na dysku. Tutaj mam sety które udało mi zgrać oraz które muszę jeszcze zgrać.\n",
    "scrape_csv_path = 'SCRAPE.csv'\n",
    "got_csv_path = 'GOT.csv'\n",
    "\n",
    "scrape_df = pd.read_csv(scrape_csv_path, sep=';', error_bad_lines=False, warn_bad_lines=True)\n",
    "got_df = pd.read_csv(got_csv_path, sep=';', error_bad_lines=False, warn_bad_lines=True)\n",
    "\n",
    "new_sets = scrape_df['set_num'].tolist()\n",
    "got_sets = got_df['Number'].tolist()\n",
    "\n",
    "# To jest lista zawierającą numery z listy 'do pobrania', które nie powtarzają się w liście \"już pobrane\"\n",
    "d_sets = [num for num in new_sets if num not in got_sets]\n",
    "\n",
    "print(len(new_sets))\n",
    "print(len(got_df))\n",
    "print(len(new_sets))\n",
    "print(len(d_sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0e9497-e4cd-4a1f-8bde-c85600550d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapowanie strony: https://brickset.com/sets/5006010-1\n",
      "Scrapowanie strony: https://brickset.com/sets/5006012-1\n",
      "No 'text' div found on the page https://brickset.com/sets/5006012-1\n",
      "Scrapowanie strony: https://brickset.com/sets/5006013-1\n",
      "No 'text' div found on the page https://brickset.com/sets/5006013-1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7700\\3255511135.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;31m# Poczekaj 2 sekundy przed kolejnym scrapowaniem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;31m# Konwersja danych do DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Funkcja do scrapowania danych z Brickset\n",
    "\n",
    "def scrap_page(url, max_retries=5, backoff_factor=60):\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            text_div = soup.find('div', class_='text')\n",
    "            \n",
    "            if text_div:\n",
    "                dt_elements = text_div.find_all('dt')\n",
    "                dd_elements = text_div.find_all('dd')\n",
    "\n",
    "                data = {'Number':'','Name':'','Type':'','Theme group':'','Theme':'','Year released':'', 'Launch/exit': '','Pieces':'','Minifigs':'','Designer':'','RRP': '', 'Age range':'','Packaging':'','Weight':'','Availability':''}\n",
    "\n",
    "                for dt, dd in zip(dt_elements, dd_elements):\n",
    "                    dt_text = dt.get_text(strip=True)\n",
    "                    dd_text = dd.get_text(strip=True)\n",
    "\n",
    "                    if dt_text == 'Number':\n",
    "                        data['Number'] = dd_text\n",
    "                    elif dt_text == 'Name':\n",
    "                        data['Name'] = dd_text\n",
    "                    elif dt_text == 'Type':\n",
    "                        data['Type'] = dd_text\n",
    "                    elif dt_text == 'Theme group':\n",
    "                        data['Theme group'] = dd_text \n",
    "                    elif dt_text == 'Theme':\n",
    "                        data['Theme'] = dd_text\n",
    "                    elif dt_text == 'Year released':\n",
    "                        data['Year released'] = dd_text \n",
    "                    elif dt_text == 'Launch/exit':\n",
    "                        data['Launch/exit'] = dd_text \n",
    "                    elif dt_text == 'Pieces':\n",
    "                        data['Pieces'] = dd_text \n",
    "                    elif dt_text == 'Minifigs':\n",
    "                        data['Minifigs'] = dd_text\n",
    "                    elif dt_text == 'Designer':\n",
    "                        data['Designer'] = dd_text \n",
    "                    elif dt_text == 'RRP':\n",
    "                        data['RRP'] = dd_text \n",
    "                    elif dt_text == 'Age range':\n",
    "                        data['Age range'] = 'x' + str(dd_text)\n",
    "                    elif dt_text == 'Packaging':\n",
    "                        data['Packaging'] = dd_text\n",
    "                    elif dt_text == 'Weight':\n",
    "                        data['Weight'] = dd_text \n",
    "                    elif dt_text == 'Availability':\n",
    "                        data['Availability'] = dd_text \n",
    "\n",
    "                return data\n",
    "            else:\n",
    "                print(f\"No 'text' div found on the page {url}\")\n",
    "                return None\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"Received 429 Too Many Requests. Retrying in {backoff_factor} seconds...\")\n",
    "            time.sleep(backoff_factor)\n",
    "            attempts += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the page {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    print(f\"Max retries reached for {url}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Pobierałam dane batchami - a) żeby strona mi zablokowała I.P. i b) często wykrzaczało się z innych powodów, internet czasem padał\n",
    "num_rows_to_scrape = 1000\n",
    "\n",
    "# Zawsze wpisywałam od nowa index batcha, który mam teraz ściągać\n",
    "start_index = 6000\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Scrapowanie danych ze wszystkich stron z listy d_sets, zaczynając od start_index\n",
    "for set_num in d_sets[start_index:]:\n",
    "    if len(all_data) >= num_rows_to_scrape:\n",
    "        break\n",
    "    page_url = f\"https://brickset.com/sets/{set_num}\"\n",
    "    print(f\"Scrapowanie strony: {page_url}\")\n",
    "    data = scrap_page(page_url)\n",
    "    if data:\n",
    "        all_data.append(data)\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Zapisuję dany batch do csv - oznaczam zakres batcha w nazwie\n",
    "df.to_csv('scrapowane_dane_6000_7000.csv', index=False)\n",
    "\n",
    "print(\"Dane zostały zapisane do pliku 'scrapowane_dane.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
